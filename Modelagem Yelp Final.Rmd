---
title: "Sistema de Recomendação de Estabelecimentos em Toronto"
author: "Marcelo Francheschini, Rafael Costa, Viviane Sanchez"
date: "6/27/2020"
institute: "Insper - Programa Avançado em Data Science 2020"
always_allow_html: true
#output: github_document
output:
  html_document: default
  toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, fig.height=5, fig.width=10)
```

# 1. Introdução

## 1.1. Objetivo

**Criar um sistema de recomendação de estabelecimentos bem avaliados em linha com o perfil do usuário.**

Os dados das seguintes bases fornecidas pelp Yelp para estudos acadêmicos serão analisados

- Business - informações e atributos dos estabelecimentos
- Users - Informações sobre o perfil do usuário no Yelp
- Reviews - Texto e atributos de avaliações dos estabelecimentos criadas pelos usuários
- Tips - Dicas adicionais e atributos deixados pelos usuários
- Check-ins - Histórico de check-ins por estabelecimento

- [Fonte de dados](https://www.yelp.com/dataset)

Foram selecinados dados de usuários e estabelecimentos abertos em Toronto para treinar um modelo de classificação binária das notas das avaliações e assim *prever se a nota de um estabelecimento que o usuário ainda não avaliou seria boa ou ruim*. A recomendação é dada conforme a maior probabilidade de o usuário dar uma nota alta para aquele lugar.

## 1.2. Tratamento da base

As bases mencionadas foram previamente tratadas em Python/PySpark. Neste relatório, serão analisadas individualmente as informações dos estabelecimentos e de usuários. Além disso, uma base contendo os dados dessas duas bases e das dicas por estabelecimento e usuário foram consolidados em uma base única para modelagem com uma rede neural. Por fim, um usuário fictício é criado para que uma recomendação seja feita. 

# 2. Análise Exploratória

## 2.1. Pacotes

```{r echo=FALSE, results= 'hide'}

library(tidyverse)
library(tidymodels)
library(tidytext)
library(skimr)
library(ggrepel)
library(factoextra)
library(vip)
library(cluster)
library(plotly)
library(zoo)
library(keras)
library(reticulate)
library(ggmap)
library(rpart)
library(partykit)
library(clue)

#rmarkdown::render("Modelagem Yelp Final.Rmd", envir=.GlobalEnv)

```

## 2.2. Leitura de arquivos individuais

```{r echo=TRUE, results= 'hide'}

yelp_bz_raw <- list.files(path = 'output/yelp_bz.csv/', 
                       pattern = "*.csv",
                       full.names = TRUE) %>% 
            map_df(~read_csv(.))

yelp_users <- list.files(path = 'output/yelp_usr.csv/', 
                       pattern = "*.csv",
                       full.names = TRUE) %>% 
            map_df(~read_csv(.))

```

## 2.3. Business

A seguir, será feita uma análise de componentes principais para entender a variabilidade da nota dos estabelcimentos conforme seus atributos, que foram codificados numericamente:

- Atributos com True/False como resposta:

    - Null/None: 0
    - False: 1
    - True: 2
    
- Atributos que possuem uma descrição das características foram substituídas por números como o exemplo:

    - Null/None: 0
    - Característica 1 - 1
    - Característica N - N

Em seguida, são removidas as colunas não numéricas (business_id, categories) para a análise e, caso exista algum valor faltando, é substituído por zero.

```{r echo=TRUE, results= 'hide'}

yelp_bz <- yelp_bz_raw %>% 
          select_if(~is.numeric(.)) %>% 
          mutate_all(~replace(., is.na(.), 0))

glimpse(yelp_bz)

```

### 2.3.1. PCA

A análise PCA é feita com a criação de uma receita pelo pacote tidymodels (`step_pca`). A base processada é extraída com a função `juice`.

```{r echo=TRUE}
  
rec_pca <- recipe(stars ~ ., yelp_bz) %>% 
  update_role(contains('id'), new_role = 'id') %>% 
  #step_date(date_rv, yelping_since_usr, features = c("dow", "month","year")) %>% 
  #step_other(categories, threshold = 0.005) %>% 
  #step_other(postal_code, threshold = 0.01) %>% 
  #step_dummy(all_nominal(), -'business_id',-'user_id',-'name_bz') %>%
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_pca(all_numeric(), -all_outcomes()) %>% 
  step_naomit(all_numeric()) %>% 
  prep()

yelp_bz_pca <- juice(rec_pca)

```

#### 2.3.1.1. Scree plot

```{r echo=TRUE, fig.width= 10, fig.height= 5}

variance_pct <- rec_pca$steps[[2]]$res

(cumsum(variance_pct$sdev^2) / sum(variance_pct$sdev^2))

fviz_eig(variance_pct, addlabels = TRUE) + 
  labs(x = "Componente Principal",
       y = "Percentual explicado da variância")

```

Nota-se que mais de 50% da variabilidade é explicada pelas 5 primeiras componentes. Suas composições serão avaliadas no próximo item.

#### 2.3.1.2. Drivers

```{r echo=TRUE, fig.width= 10, fig.height= 5}

tidy_pca <- tidy(rec_pca, 2)

tidy_pca %>%
  filter(component %in% paste0("PC", 1:6)) %>%
  group_by(component) %>%
  top_n(15, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Valor absoluto da contribuição",
    y = NULL, fill = "Valor > 0")


```

Na primeira componente, os pesos são igualmente distribuídos, o que indica que todos os atributos tem impacto semelhante na maior parte da variabilidade. 

Pela segunda componente, no entanto, observa-se que a existência de um local para deixar o casaco, ser permitido fumar e ser um bom local para dançar são mais relevantes, assim como a localização (PC6).
Além disso, cobrança de rolha e necessidade de levar a bebida também são drivers importantes, pois aparecem em mais de uma componente. Importante notar que o nível de preço do estabelecimento aparece apenas na 5a componente.

#### 2.3.1.3.Contrastes

Observa-se a interação entre as principais variáveis das duas primeiras compomentes:

```{r echo=TRUE, fig.width= 10, fig.height= 8}

variance_pct %>% 
  fviz_pca_var(axes = c(1,2), col.var="contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))


```

Os maiores contrastes são entre a modalidade de atendimento dos restaurante: Apenas delivery ou com reservas.

## 2.4. Users

Para fazer recomendações baseadas no perfil do usuário, é necessários agrupá-los de alguma forma. Foram testados os algoritmos de clusterização hierárquica e k-means, mas apenas o último trouxe resultados interpretáveis e satisfatórios.

### 2.4.1. K-Means

Para definir o número de clusters ideal, é feita uma análise de sensibilidade e aplicado o [algoritmo kmeans no formato tidy]((https://www.tidymodels.org/learn/statistics/k-means/))

```{r echo=TRUE, warning=FALSE}

set.seed(123)

glimpse(yelp_users)

yelp_pad <- yelp_users %>% 
              select(-user_id) %>% 
              scale()

kclusts <- tibble(k = 1:30) %>%
  mutate(kclust = map(k, ~kmeans(yelp_pad, .x)),
        tidied = map(kclust, tidy),
        glanced = map(kclust, glance),
        augmented = map(kclust, augment, yelp_pad)
        )

clusters <- kclusts %>%
  unnest(cols = c(tidied))

assignments <- kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- kclusts %>%
  unnest(cols = c(glanced))

```


```{r echo=TRUE}

### Cotovelo

clusterings %>% 
  ggplot(aes(k, tot.withinss)) + 
    geom_point(size = 3) + 
    geom_line() + 
    labs(y = "total within sum of squares", x = "k") +
    scale_x_continuous(breaks = 1:30)

```

Pelo gráfico do cotovelo, poderiam ser selecionado um número de clusters (k) de 11 a 17. a seguir é possível ver a eveolução do algoritmo de clusterização em relação às dicas deixadas por usuário e o quanto foram elogiadas pela comunidade.
 
```{r echo=TRUE, fig.width= 10, fig.height= 5}

#k-means
assignments %>% 
  filter(k %in% paste0(10:20)) %>%
  ggplot(aes(x = tips_counter, y = total_compliments)) +
  geom_point(aes(color = .cluster), alpha = 0.5) + 
  facet_wrap(~ k, nrow = 3)

```

Para a classificação final dos usuário, será feita novamente a clusterização, mas considerando apenas o número de clusters `k` ideal.

```{r echo=TRUE}

set.seed(123)
kmeans_usr <-  kmeans(yelp_pad, 11)

yelp_usr_cluster <- yelp_users %>% 
          mutate(cluster_usr = kmeans_usr$cluster)

#glimpse(yelp_usr_cluster)

```

Por fim, observa-se claramente a divisão dos usuários em relação ao tempo na plataforma e a nota média do usuário. Além disso, nota-se diferentes camadas em relação o número de fãs, diferenciando os usuários que teriam um pontecial de impactar o negócio ao deixar uma avaliação.

```{r echo=TRUE, fig.width= 10, fig.height= 10}

plot_ly(yelp_usr_cluster, x = ~year_since, 
               y = ~average_stars,
               z = ~fans, color = ~cluster_usr,
              text = ~paste('Cluster: ', cluster_usr)) %>% 
  add_markers() %>% 
  layout(scene = list(xaxis = list(title = 'No Yelp desde'),
                                   yaxis = list(title = 'Nota Média'),
                                   zaxis = list(title = 'Quantidade de fãs')))

```

A base é então transferida para csv para incluir o número do cluster dos usuários na base final da modelagem.

```{r echo=TRUE}

yelp_usr_cluster %>% 
          select(user_id, cluster_usr) %>%
          write.csv(file = "output/usr_cluster.csv")

```

Como próximos passos, seria interessante entender melhor as características de cada cluster. No relatório em Python é feita uma análise dos textos das avaliações para cada cluster.

Para classificar usuários que não estão na base selecionada, foi feita inicialmente uma árvore de classificação.

### 2.4.2. Modelo para definição do cluster do usuário

```{r echo=TRUE}

user_cluster_tree <- yelp_usr_cluster %>% 
                    select(-user_id) %>% 
                    rpart(cluster_usr ~ ., data = .)

plot_arvore <- as.party(user_cluster_tree)

#plot(plot_arvore)

```


# 3. Modelagem

## 3.1 Leitura da base final

Após consolidação da base em Python, é feita a leitura da base e substituição da variável resposta para binária:
- Notas maiores ou iguais a 4 - boas (1)
- Notas menores do que 4 - ruim (0)

Além disso, são mantidas apenas as variáveis numéricas para treinamento da rede neural.

```{r echo=TRUE, results= 'hide'}

yelp_raw <- list.files(path = 'output/yelp.csv/', 
                       pattern = "*.csv",
                       full.names = TRUE) %>% 
            map_df(~read_csv(.))

glimpse(yelp_raw)

```

```{r}

yelp_rv <- yelp_raw %>% 
  #mutate(line = row_number()) %>% 
  select(-'year_rv') %>% 
  mutate(stars_rv = replace(stars_rv >= 4,1,0)) %>% 
  select_if(is.numeric) #%>% sample_frac(0.50)

glimpse(yelp_rv)

```

## 3.2 Bases de Treino e Teste

A base é divida em treino, validação e teste com a função `split` do pacote tidymodels.

```{r}
split <- initial_split(yelp_rv, prop = 0.8 , strata = stars_rv)

train_val <- training(split)


split_val <- initial_split(train_val, prop = 0.5, strata = stars_rv)

yelp_train <- training(split_val)
yelp_val <- testing(split_val)
yelp_test <- testing(split)

```

Calcula-se então a média e desvio padrão da base de treino para utilizar na normalização. (Chollet)

```{r}
mean <- yelp_train %>% 
        select(-stars_rv) %>% 
        apply(., 2, mean) 

std <- yelp_train %>% 
        select(-stars_rv) %>% 
        apply(., 2, sd)
```

Por fim, as bases são normalizadas e transformadas em matriz para otimizar os cálculos na rede neural

```{r}
x_train <- yelp_train %>% 
            select(-stars_rv) %>% 
            scale(center = mean, scale = std) %>% 
            as.matrix()

dim(x_train)

y_train <- yelp_train %>% 
            select(stars_rv) %>% 
            as.matrix()

x_val <-  yelp_val %>% 
            select(-stars_rv) %>% 
            scale(center = mean, scale = std) %>% 
            as.matrix()

dim(x_val)

y_val <- yelp_val %>% 
            select(stars_rv) %>% 
            data.matrix()

dim(x_val)

x_test <- yelp_test %>% 
          select(-stars_rv) %>% 
          scale(center = mean, scale = std) %>% 
          as.matrix()

dim(x_test) 

y_test <- yelp_test %>% 
            select(stars_rv) %>% 
            data.matrix()

```

## 3.3. Rede Neural

Foram testadas diferentes estruturas de rede neural. De qualquer forma, a última camada possui uma função de ativação `sigmoid` para que a resposta seja uma probabilidade de acerto.

```{r fig.height=5, fig.width=10}

rm(yelp_nn)

yelp_nn <- keras_model_sequential() %>% 
  layer_dense(units = 30, activation = "tanh", input_shape = ncol(x_train)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>%
  #layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>%
  #layer_dense(units = 6, activation = "softmax")
  layer_dense(units = 1, activation = "sigmoid")

yelp_nn %>% 
  compile(optimizer = "rmsprop", 
          #loss = "sparse_categorical_crossentropy", 
          loss = "binary_crossentropy",
          metrics = c("accuracy"))


history <- yelp_nn %>% 
  fit(x_train, y_train, 
      epochs = 40, batch_size = 512, 
      validation_data = list(x_val, y_val))

plot(history)

#keras::get_weights(yelp_nn)

(results <- yelp_nn %>% evaluate(x_test, y_test))

```

Foi adicionada uma camada de dropout para diminuir o overfit do modelo. Observa-se que foi efeciente, pois a perda da base de validação não ultrappassa a perda da base de treino. Ainda assim, mais testes de estruturas e otimização poderiam ser feitos para melhorar a performance do modelo.

## 3.4. Desempenho do modelo

Com o modelo pronto, é feito um teste na respectiva base. Sua performance é calculada com a área sob a curva ROC.

```{r}

resultados <- tibble(observado = factor(y_test)) %>% 
  bind_cols(data.frame(prob = predict(yelp_nn, as.matrix(x_test))))

resultados %>% 
  roc_auc(observado, prob)

resultados %>% 
  roc_curve(observado, prob) %>% 
  autoplot()

```

Pelo gráfico, observa-se que o modelo atingiu um desempenho satisfatório na base de teste, considerando que é um problema de recomendação. A seguir, a matriz de confusão do modelo, considerando o corte de 0.50, pois é preferível minimizar os falsos negativos (recomendar um estabelecimento que o usuário não irá gostar) do que os falsos positivos (deixar de recomendar um restaurante que o usuário irá gostar, que pode ser um que ele já tenha avaliado).

```{r}

resultados %>% 
  mutate(.pred = if_else(prob >= 0.5,1,0)) %>% 
  mutate(.pred = as.factor(.pred)) %>% 
  conf_mat(truth = observado, estimate = .pred) %>% 
  autoplot()

?case

```


# 4. Recomendação

## 4.1. Usuário criado

Abaixo é criado um usuário com as mesmas informações existentes na base de usuários de forma aleatória. Para definir seu cluster, é utilizada a árvore criada anteriormente.

```{r}

glimpse(yelp_usr_cluster)

rm(user)

compliment_max <- 100

user <- tibble(user_id = 'random_user',
               average_stars = round(runif(1, 1.0, 5),2),
               compliment_cool = ceiling(runif(1,0, compliment_max)),
               compliment_cute = ceiling(runif(1,0, compliment_max)),
               compliment_funny = ceiling(runif(1,0, compliment_max)),
               compliment_hot  = ceiling(runif(1,0, compliment_max)),
               compliment_list = ceiling(runif(1,0, compliment_max)),
               compliment_more = ceiling(runif(1,0, compliment_max)),
               compliment_note = ceiling(runif(1,0, compliment_max)),
               compliment_photos = ceiling(runif(1,0, compliment_max)),
               compliment_plain = ceiling(runif(1,0, compliment_max)),
               compliment_profile = ceiling(runif(1,0, compliment_max)),
               compliment_writer = ceiling(runif(1,0, compliment_max)),
               cool = ceiling(runif(1,0, compliment_max)),
               elite_count = 0,
               fans = ceiling(runif(1,0, compliment_max)),
               friends_count = ceiling(runif(1,0, compliment_max)),
               funny = ceiling(runif(1,0, compliment_max)),
               review_count_usr = ceiling(runif(1,0,compliment_max)),
               useful = ceiling(runif(1,0, compliment_max)),
               year_since = ceiling(runif(1,2004, 2019)),
               tips_counter = ceiling(runif(1,0, compliment_max)),
               total_compliments = ceiling(runif(1,0, compliment_max))
                )

## criação aleatória do número de anos que o usuário foi elite
user$elite_count <- ceiling(runif(1,0, (2020-user$year_since)))

#encontra o número do cluster em que o usuário se encaixa
user$cluster_usr <- user_cluster_tree %>%
      predict(user) %>% 
      ceiling()

```

Usuário criado:

```{r}

glimpse(user)

```

Em seguida, a partir do número de reviews gerado, é selecionada aleatóriamente a mesma quantidadade de estabelecimentos da base business e atribuídas notas de review aleatórias. Depois, as informações do usuário e dos estabelcimentos são consolidadas em uma base única, como se fosse uma amostra da base original.

```{r}

# seleção aleatória de estabelecimentos e notas atribuídas a cada um baseado no número de reviews

n_reviews <- user$review_count_usr

reviewed_usr <- tibble(business_id = sample(yelp_bz_raw$business_id, 
                                            n_reviews), #seleçao aleatória de estabelecimentos
                                            stars_rv = ceiling(runif(n_reviews, 1.0, 5)), #nota
                                            year_rv = ceiling(runif(n_reviews, 2009, 2019)), #ano da review
                                            )
user_hist <- user %>% 
            bind_rows(replicate(n_reviews-1, user, simplify = FALSE)) %>% #replica as informações do usuário
            bind_cols(reviewed_usr) %>% #junta os estabelecimentos e notas dadas
            left_join(., yelp_bz_raw, by = 'business_id') #junta as informações dos estabelecimentos

#glimpse(user_hist)


```

## 4.2. Função para recomendação

Para gerar as recomendações, foi criada uma função para selecionar os possíveis restaurantes que o usuário gostaria de ir e calcular a probabilidade de uma boa avaliação. Como dados de entrada, devem ser fornecidas as informações do perfil do usuário no yelp e as avaliações de estabelecimentos já visitados.

A partir dessas informações, é gerada a base `to_go` que, a partir do cluster do usuário, filtra os estabelecimentos que ele poderia ir de acordo com as boas avaliações feitas por usuários de perfil semelehante (do mesmo cluster)

Em seguida, na base `to_review`, são cruzadas as informações de avaliações do usuário com a base anterior para criar uma matriz contendo as informações do usuário e dos estabelecimentos que ele ainda não avaliou. Essa base é então normalizada e transoformada em matriz para input no modelo pela base `user_x_test`. 

As previsões de probabilidade do usuário dar uma nota maior ou igual a 4 para os estabelcimentos que ele ainda não visitou são então armazenadas no vetor `predictions`, que é então adicionado à base `to_review`, criando uma tabela única que será utilizada como fonte para as recommendações. Por fim, são filtrados os estabelecimentos com probabilidade maior que 50%.

```{r}

recomm_f <- function(user, reviewed_usr){
  
  to_go <- yelp_raw %>% 
    filter(stars_rv >= 4) %>% 
    filter(cluster_usr == user$cluster_usr) %>% 
    select(business_id) %>% 
    distinct() 
  
  n_go <- nrow(to_go)
  
  #filtra todos os estabelecimentos do cluster do usuário e junta as informações para modelagem
  to_review <- user %>% 
            bind_rows(replicate(n_go-1, user, simplify = FALSE)) %>% #replica as informações do usuário
            bind_cols(to_go) %>% #junta os estabelecimentos e notas dadas
            left_join(., yelp_bz_raw, by = 'business_id')
  
  #prepara a base para o modelo
  user_x_test <- to_review %>% 
          select_if(is.numeric) %>% 
          #select(-stars_rv) %>% 
          scale(center = mean, scale = std) %>% 
          as.matrix()

  #aplica a base no modelo
  predictions <- as_tibble(predict(yelp_nn, user_x_test))
  
  
  #seleciona as principais recomendações
  recommendation <- to_review %>% 
    bind_cols(pred = predictions) %>% 
    anti_join(., reviewed_usr, by = 'business_id') %>% 
    filter(V1 > 0.5)

}

```

### 4.2.1. Recomendação para usuário criado

A função é então aplicada ao usuário criado com informações aleatórias. Os 5 estabelecimentos com maior probabilidade de avaliação positiva e sua localização são os seguintes.

```{r eval=FALSE, include=FALSE}

recommendation_new <- recomm_f(user, reviewed_usr)

recommendation_new %>% 
  arrange(-V1) %>% 
  select(name, categories, V1)

Í#mapa com as top 5 recomendações

top_5 <- recommendation_new %>% 
    top_n(5, V1) %>%
    arrange(-V1) %>% 
    mutate(rank = as.factor(row_number()))

top_5 %>% 
  select(name, categories, V1)

qmplot(longitude, latitude, data = top_5, 
       maptype = "toner-background", 
       color = rank,
       size = V1)

```

## 4.3. Usuário aleatório da base

Para validar as recomendações, é feito o teste também com um usuário aleatório da base de teste.

```{r}

n <- ceiling(runif(1,1,nrow(yelp_test)))


(random_user <- yelp_raw[n,]$user_id)


user2 <- yelp_usr_cluster %>% 
            filter(user_id == random_user)

glimpse(user2)

reviewed_usr2 <- yelp_raw %>% 
  filter(user_id == random_user)

rec_user <- recomm_f(user2,reviewed_usr2)
  
```

### 4.3.1. Recomendação para usuário da base

```{r}

#top 5 recomendações

  rec_user %>% 
    top_n(5, V1) %>%
    arrange(V1) %>% 
    mutate(rank = as.factor(row_number())) %>% 
    ggplot(aes(x = V1, y = name)) +
    geom_col() +
    labs(x = "Probabilidade de avaliação positiva",
    y = 'Recomendação')
  

```

```{r}

top_5 <- rec_user %>% 
    top_n(5, V1) %>%
    arrange(-V1) %>% 
    mutate(rank = as.factor(row_number()))

top_5 %>% 
  select(name, categories, V1)

qmplot(longitude, latitude, data = top_5, 
       maptype = "toner-background", 
       color = rank,
       size = V1)


```


### 4.3.4.Recomendação por categoria

O ideal seria fornecer recomendações de acordo com o o que o usuário procura. Por isso, é mostrado abaixo os top 5 estabelecimentos de diferentes categorias.

```{r}

rec_user %>% 
  select(name, categories, V1) %>% 
  unnest_tokens(category, categories) %>% 
  filter(category %in% c('food','restaurants','bars','pub')) %>% 
  group_by(category) %>%
  mutate(pred_avg = mean(V1)) %>% 
  arrange(desc(V1)) %>% 
  unique() %>% 
  slice(1:5) %>%
  ggplot(aes(V1, name, fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, scales = 'free') +
  scale_x_continuous() +
  scale_y_reordered() +
  labs(x = 'Probabilidade de boa avaliação')

```


# 5. Conclusão

Foram avaliadas todas as bases do dataset, mas nem todas as informações disponíveis foram utilizadas no modelo.
Após entender como os atributos dos estabelecimentos impactam na nota, foi feita uma clusterização dos usuários para identificar usuários com perfis semelhantes e utilizar os locais frequentados como base para as recomendacões. Além de fazer uma melhor calibração no modelo, uma possibilidade de otimizar as recomendações seria utilizar as informações de movimento dos estabelecimentos e cruzá-la com os horários em que as avaliações foram feitas, por exemplo.

# 6.  Referências

- Neumann, D. Material de aula do cursos Big Data e Computação em Nuvem
- Mendonça, T. Material de aula do curso Modelagem Preditiva Avançada
- [Fernandez, P. Marques. P. Data Science, Marketing and Business](https://datascience.insper.edu.br/datascience.pdf)
- [Rahimi, S.; Mottahedi, S.; Liu, X. The Geography of Taste: Using Yelp to Study Urban Culture. ISPRS Int. J. Geo-Inf. 2018, 7, 376.](https://www.mdpi.com/2220-9964/7/9/376?type=check_update&version=1)
- [Chollet, F. et al, Deep Learning with R](https://www.manning.com/books/deep-learning-with-r)
- [Silge, J.](https://juliasilge.com/blog/sherlock-holmes-stm/)
- https://www.datanovia.com/en/lessons/clustering-distance-measures/
- [K-means clustering with tidy data principles](https://www.tidymodels.org/learn/statistics/k-means/)

- Arquivos disponíveis no [repositório](https://github.com/sanchezvivi/pads-yelp)
