---
title: "Integradora Yelp"
author: "Marcelo Francheschini, Rafael Costa, Viviane Sanchez"
date: "6/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)
library(tidymodels)
library(tidytext)
library(skimr)
library(ggrepel)
library(ggdendro)
library(factoextra)
library(vip)
library(doParallel)
library(leaps)

library(tidytext)
library(tm)
library(wordcloud)
library(topicmodels)
library(drlib)
library(quanteda)
library(stm)

library(keras)
library(reticulate)

```


## Leitura de arquivos

```{r eval=FALSE, include=FALSE}

yelp_raw <- list.files(path = 'output/yelp.csv/', 
                       pattern = "*.csv",
                       full.names = TRUE) %>% 
            map_df(~read_csv(.))


yelp_dist_raw <- list.files(path = 'output/yelp_dist.csv/', 
                       pattern = "*.csv",
                       full.names = TRUE) %>% 
            map_df(~read_csv(.))

yelp_words <- list.files(path = 'output/yelp_words.csv/', 
                       pattern = "*.csv",
                       full.names = TRUE) %>% 
            map_df(~read_csv(.))

skim(yelp_raw)


```


## Análise de reviews e tips

[FONTE](https://juliasilge.com/blog/sherlock-holmes-stm/)

```{r}

yelp_corpus <- yelp_words %>% 
  #select(review_tip) %>% 
  mutate(line = row_number()) %>% 
  unnest_tokens(word, review_tip) %>% 
  anti_join(stop_words) %>% 
  filter(!word %in% c(0,1,2,3,4,5,6,7,8,9)) %>% 
  filter(!word %in% c('food','service','time','restaurant'))
  
  
glimpse(stop_words)

yelp_corpus %>%
    count(word, sort = TRUE)

```


### Wordcloud
Palavras mais utilizadas nas reviews boas:
```{r}

corpus %>% 
  filter(stars >= 4 ) %>% 
  filter(!word %in%  c('food','service')) %>% 
  count(word, sort = TRUE) %>% 
  top_n(100, n) %>% 
  with(wordcloud(word, n, random.order = FALSE, 
       colors = brewer.pal(8, "Dark2")))
```


Palavras mais utilizadas nas reviews não boas:
```{r}

corpus %>% 
  filter(stars < 4 ) %>% 
  filter(!word %in%  c('food','service')) %>% 
  count(word, sort = TRUE) %>% 
  top_n(100, n) %>% 
  with(wordcloud(word, n, random.order = FALSE, 
       colors = brewer.pal(8, "Dark2")))

```


## Modelo de tópicos

### Frequência
```{r}

yelp_tf_idf <- yelp_corpus %>%
    count(stars, word, sort = TRUE) %>%
    bind_tf_idf(word, stars, n) %>%
    arrange(-tf_idf) %>%
    group_by(stars) %>%
    top_n(10) %>%
    ungroup

yelp_tf_idf %>%
    mutate(word = reorder_within(word, tf_idf, stars)) %>%
    ggplot(aes(word, tf_idf, fill = stars)) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~stars, scales = "free") +
    scale_x_reordered() +
    coord_flip() +
    #theme(strip.text=element_text(size=11)) +
    labs(x = NULL, y = "tf-idf",
         title = "Highest tf-idf words by review stars"
         #subtitle = "Individual stories focus on different characters and narrative elements"
         )

```

### Modelo de tópicos
```{r}

yelp_dfm <- yelp_corpus %>%
    count(stars, word, sort = TRUE) %>%
    cast_dfm(stars, word, n)

topic_model <- stm(yelp_dfm, K = 5, 
                   verbose = FALSE, init.type = "Spectral")

yelp_sparse <- yelp_corpus %>%
    count(stars, word, sort = TRUE) %>%
    cast_sparse(stars, word, n)

topic_model_sparse <- stm(yelp_sparse, K = 5, 
                   verbose = FALSE, init.type = "Spectral")

td_beta <- tidy(topic_model)

td_beta_sparse <- tidy(topic_model_sparse)

td_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Same words are associated with different topics")

```


### Probabilidades do documento para cada tópico

```{r}


td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(yelp_dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 3) +
  labs(title = "Distribution of document probabilities for each topic",
       subtitle = "Each topic is associated with 1-3 stars",
       y = "Number of stars", x = expression(gamma))


```


# Clusters
## yelp - dist

```{r}

yelp_euc <- yelp_dist_raw %>%
        select(-user_id) %>% 
        select_if(colSums(.) != 0) %>% 
        mutate_all(~replace(., is.na(.), 0)) %>% 
        as.matrix()

euc_hclust <- (hclust(get_dist(t(yelp_euc), method = 'euclidean')))

fviz_dend(euc_hclust, cex = 0.5, k = 50,
          main = "Categorias em Toronto - Euclidiana",
          color_labels_by_k = TRUE, horiz = TRUE)

view(yelp_dist_raw)

yelp_jaccard <- yelp_dist_raw %>%
        select(-user_id) %>% 
        select_if(colSums(.) != 0) %>%
        mutate_all(~replace(., is.na(.), 0)) %>% 
        mutate_all(., ~replace(. >= 3, 1,0)) %>% 
        #mutate(user_id = yelp_dist_raw$user_id) %>% 
        as.matrix()

jac_hclust <- hclust(get_dist(t(yelp_jaccard), method = 'binary'))

fviz_dend(jac_hclust, cex = 0.5, k = 10,
          main = "Categorias em Toronto - Jaccard",
          color_labels_by_k = TRUE, horiz = TRUE)

```



```{r}

##########   Hierarchical Clustering - Dist Euclidiana    ##########

ggdendrogram(euc_hclust, rotate = TRUE, labels = TRUE) +
    labs(title = "Categorias em Toronto - Euclidiana")

# Cortar A clusterização com base no número de clusters
euc_clust <- cutree(euc_hclust, k = 20)



##########  Hierarchical Clustering - Dist Jaccard  ##########

ggdendrogram(jac_hclust, rotate = TRUE, labels = TRUE, color = order) +
    labs(title = "Categorias em Toronto - Jaccard")


fviz_dend(jac_hclust, cex = 0.5, k = 20,
          main = "Categorias em Toronto",
          color_labels_by_k = TRUE, horiz = TRUE) + 
          theme_void()



# Cortar A clusterização com base no número de clusters
jc_clust <- cutree(jac_hclust, k = 20)


```

## hclust

```{r}

yelp_matrix <- yelp_juice %>% 
  drop_na() %>% 
  select(-user_id, -business_id, -name_bz, -date, -yelping_since) %>% 
  as.matrix()

kclust <- kmeans(yelp_matrix, 5)

summary(kclust)

kclusts <- 
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(yelp_matrix, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )

```


```{r}

    yelp_hclust <- yelp_matrix %>%
        get_dist(method = "euclidean")
    
        #hclust(method = "complete")

    
       yelp_hclust <- yelp_juice %>%
        select(-user_id, -business_id, -date, -yelping_since) %>% 
        get_dist(method = "manhattan") %>%  
        hclust(method = "complete")


```



## Yelp -  base completa 

```{r}

skim(yelp_raw)

yelp <- yelp_raw %>% 
        select(-review_tip,-review_id) %>% 
        mutate(yelping_since = as.Date(yelping_since),
         date = as.Date(date))
  

skim(yelp)

```

### Latitude vs Longitude

```{r}

gg_lat_lon <- function(col_ref, title_string){
      yelp %>% 
        ggplot(aes(longitude, latitude, color = col_ref)) +
        geom_point(size = 0.5, alpha = 0.7) +
        scale_color_gradient(low = 'skyblue', high = 'gold', labels = scales::label_number_si()) +
        labs(title = title_string)
}

```

```{r}

gg_lat_lon(yelp$stars_usr, 'Average Stars by user')

```

```{r}

gg_lat_lon(yelp$stars, 'Review Stars')

```

```{r}

gg_lat_lon(yelp$stars_bz, 'Average Stars by Business')

```

```{r}

gg_lat_lon(yelp$review_count, 'Review count by Business')

```


### Split

```{r}

split <- initial_split(yelp, prop = 0.8)

train <- training(split)
test <- testing(split)

```


### Recipe

```{r}

rec <- recipe(~ ., train) %>% 
  update_role(business_id, user_id, name_bz, new_role = 'id') %>% 
  step_date(date, yelping_since, features = c("dow", "month","year")) %>% 
  step_other(categories, postal_code, threshold = 0.05) %>% 
  #step_other(postal_code, threshold = 0.01) %>% 
  step_dummy(all_nominal(), - 'business_id',-'user_id',-'name_bz') %>%
  step_normalize(all_numeric()) %>% 
  step_naomit(all_predictors())

yelp_juice <- juice(prep(rec))

skim(yelp_juice)


```

### K-means
```{r}

yelp_km <- yelp_juice %>% 
  select(-business_id,-user_id, -name_bz,-date,-yelping_since)

cotovelo <- function(k) kmeans(yelp_km, k)$tot.withinss


estudo <- tibble(k = 1:30) %>% #estudo até 15 clusters
            mutate(w = map_dbl(k, cotovelo))

estudo %>% 
  ggplot(aes(k, w)) + 
    geom_point(size = 3) + 
    geom_line() + 
    labs(y = "total within sum of squares", x = "k") +
    scale_x_continuous(breaks = 1:30)

```


```{r}
set.seed(123)

kmedias <- kmeans(yelp_km, 7)

yelp_juice <- yelp_juice %>% 
          mutate(cluster = kmedias$cluster)


yelp_juice %>% 
  mutate(cluster = as.factor(cluster)) %>% 
  ggplot(aes(longitude, latitude, color = cluster)) + 
    geom_point(size = 3, alpha = .5) + 
    theme(legend.position = "top")


```


```{r}

set.seed(123)

kclusts <- tibble(k = 1:20) %>%
  mutate(kclust = map(k, ~kmeans(yelp_km, .x)),
        tidied = map(kclust, tidy),
        glanced = map(kclust, glance),
        augmented = map(kclust, augment, yelp_km)
        )

clusters <- kclusts %>%
  unnest(cols = c(tidied))

assignments <- kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- kclusts %>%
  unnest(cols = c(glanced))

assignments %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)



```



```{r}

res_gap <- clusGap(yelp_km, 
                   FUN = kmeans, 
                   d.power = 2,
                   nstart = 30, 
                   iter.max = 30,
                   K.max = 20, 
                   B = 50)

fviz_gap_stat(res_gap)

```


### PCA

```{r}

skim(train)

rec_pca <- recipe(stars ~ ., train) %>% 
  update_role(business_id, user_id, name_bz, new_role = 'id') %>% 
  step_date(date, yelping_since, features = c("dow", "month","year")) %>% 
  #step_other(categories, threshold = 0.005) %>% 
  #step_other(postal_code, threshold = 0.01) %>% 
  #step_dummy(all_nominal(), -'business_id',-'user_id',-'name_bz') %>%
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_pca(all_numeric(), -all_outcomes()) %>% 
  step_naomit(all_numeric()) %>% 
  prep()


yelp_pca <- juice(rec_pca)

skim(yelp_pca)

skim(yelp)

#pca_train  <- juice(rec_pca) 

rec_pca$steps[[3]]$res

  
```


### Scree Plot

```{r}

variance_pct <- rec_pca$steps[[3]]$res

(cumsum(variance_pct$sdev^2) / sum(variance_pct$sdev^2))

fviz_eig(variance_pct, addlabels = TRUE) + 
  labs(x = "Componente Principal",
       y = "Percentual explicado da variância")

```

### Componentes Principais

```{r}

tidy_pca <- tidy(rec_pca, 3)

tidy_pca %>% 
  filter(component %in% paste0("PC", 1:5)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component) +
  labs(y = NULL)

```


### Principais contribuições

```{r}
tidy_pca %>%
  filter(component %in% paste0("PC", 1:5)) %>%
  group_by(component) %>%
  top_n(8, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Valor absoluto da contribuição",
    y = NULL, fill = "Positiva?")
```


## Contrastes

```{r}

variance_pct %>% 
  fviz_pca_var(axes = c(1,2), col.var="contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

```


### Rede Neural

```{r}

set.seed(123)

idx <- sample(c(1, 2, 3), size = nrow(yelp_pca), replace = TRUE)
  
x_tr <- select(yelp_pca[idx == 1, ], -stars) %>% 
          as.matrix()
          
y_tr <- as.numeric(yelp_pca$stars[idx == 1])


x_val <- select(yelp_pca[idx == 2, ], -stars) %>% 
          as.matrix()

y_val <- as.numeric(yelp_pca$stars[idx == 2])


x_test <- select(yelp_pca[idx == 3, ], -stars) %>% 
          as.matrix()

y_test <- as.numeric(yelp_pca$stars[idx == 3])


```


```{r}

## Define

rm(network)

network <- keras_model_sequential() %>% 
  layer_dense(units = 2, activation = "tanh", input_shape = ncol(yelp_pca)) %>%
  layer_dense(units = 4, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

network %>% 
  compile(optimizer = "rmsprop", 
          loss = "binary_crossentropy", 
          metrics = c("accuracy"))


```

```{r}

network %>% 
  fit(x_tr, y_tr, epochs = 80, batch_size = 16, 
      validation_data = list(x_val, y_val))


keras::get_weights(network)



```


```{r}





```



# Referências

- Neumann, D. Material de aula do cursos Big Data e Computação em Nuvem
- Mendonça, T. Material de aula do curso Modelagem Preditiva Avançada
- [Fernandez, P. Marques. P. Data Science, Marketing and Business](https://datascience.insper.edu.br/datascience.pdf)
- [Rahimi, S.; Mottahedi, S.; Liu, X. The Geography of Taste: Using Yelp to Study Urban Culture. ISPRS Int. J. Geo-Inf. 2018, 7, 376.](https://www.mdpi.com/2220-9964/7/9/376?type=check_update&version=1)
- [Silge, J.](https://juliasilge.com/blog/sherlock-holmes-stm/)




